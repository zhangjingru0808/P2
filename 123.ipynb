{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr8mVDEhfemn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_b2YI4Hfems"
      },
      "outputs": [],
      "source": [
        "# ########################################CUSTOM DATASET ################################################################################\n",
        "# ########################################CUSTOM DATASET ################################################################################\n",
        "# ########################################CUSTOM DATASET ################################################################################\n",
        "def one_hot_encoder(sequence):\n",
        "    # Define dictionary to map nucleotides to one-hot encoding\n",
        "    #nucleotide_map = {'A': [1, 0, 0, 0], 'T': [0, 1, 0, 0], 'G': [0, 0, 1, 0], 'C': [0, 0, 0, 1],'a': [1, 0, 0, 0], 't': [0, 1, 0, 0], 'g': [0, 0, 1, 0], 'c': [0, 0, 0, 1]}\n",
        "    A = []\n",
        "    T = []\n",
        "    C = []\n",
        "    G = []\n",
        "    for s in sequence:\n",
        "        if s.upper() == 'A':\n",
        "            A.append(1)\n",
        "            T.append(0)\n",
        "            C.append(0)\n",
        "            G.append(0)\n",
        "        elif s.upper() == 'T':\n",
        "            A.append(0)\n",
        "            T.append(1)\n",
        "            C.append(0)\n",
        "            G.append(0)\n",
        "        elif s.upper() == 'C':\n",
        "            A.append(0)\n",
        "            T.append(0)\n",
        "            C.append(1)\n",
        "            G.append(0)\n",
        "        elif s.upper() == 'G':\n",
        "            A.append(0)\n",
        "            T.append(0)\n",
        "            C.append(0)\n",
        "            G.append(1)\n",
        "\n",
        "        elif s.upper() == 'N':\n",
        "            A.append(0)\n",
        "            T.append(0)\n",
        "            C.append(0)\n",
        "            G.append(0)\n",
        "\n",
        "    encoded_sequence = []\n",
        "    encoded_sequence.append(A)\n",
        "    encoded_sequence.append(T)\n",
        "    encoded_sequence.append(C)\n",
        "    encoded_sequence.append(G)\n",
        "    return np.array(encoded_sequence)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnf_2BRufems"
      },
      "outputs": [],
      "source": [
        "def read_sequences_from_file(filename):\n",
        "    sequences = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            # Split label and sequence\n",
        "            label, sequence = line.strip().split(' ', 1)\n",
        "            sequences.append((int(label), sequence))\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tntnASAbfems"
      },
      "outputs": [],
      "source": [
        "def process_sequences(filename):\n",
        "    sequences = read_sequences_from_file(filename)\n",
        "    #c = 0\n",
        "    processed_data = []\n",
        "    for label, sequence in sequences:\n",
        "        # One-hot encode the sequence\n",
        "        one_hot_encoded_sequence = one_hot_encoder(sequence)\n",
        "        # Append label and one-hot encoded sequence\n",
        "        one_hot_encoded_sequence = np.array(one_hot_encoded_sequence)\n",
        "        processed_data.append((label, one_hot_encoded_sequence))\n",
        "\n",
        "\n",
        "    return processed_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72fh3fAhfemq",
        "outputId": "9495a1a2-bbd8-40de-bc9d-6dc353039743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 1, 28, 28)\n",
            "(16, 1, 28, 28)\n",
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n",
            "\n",
            "torch.Size([32, 1, 28, 28])\n",
            "torch.float32\n",
            "32\n",
            "\n",
            "int32\n",
            "torch.int32\n"
          ]
        }
      ],
      "source": [
        "# Generating random data\n",
        "random_train_data = np.random.rand(32,1,28, 28)\n",
        "print(random_train_data.shape)\n",
        "random_test_data = np.random.rand(16,1,28, 28)\n",
        "print(random_test_data.shape)\n",
        "\n",
        "\n",
        "# Converting the data to tensor type and floating point type\n",
        "tensor_train_data = torch.from_numpy(random_train_data).float()\n",
        "tensor_test_data = torch.from_numpy(random_test_data).float()\n",
        "print(type(tensor_train_data))\n",
        "print(type(tensor_train_data))\n",
        "print()\n",
        "print(tensor_train_data.shape)\n",
        "print(tensor_test_data.dtype)\n",
        "print(len(tensor_train_data))\n",
        "\n",
        "\n",
        "# Creating random binary labels. and converting it to tensor\n",
        "print()\n",
        "label_test = np.random.choice([0, 1], size=len(tensor_test_data))\n",
        "label_train = np.random.choice([0, 1], size=len(tensor_train_data))\n",
        "print(label_train.dtype)\n",
        "label_test = torch.from_numpy(label_test)\n",
        "label_train = torch.from_numpy(label_train)\n",
        "print(label_test.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZRg1Soofemr",
        "outputId": "a1df9947-918a-4971-e5dc-60cc3bbacbcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here <class '__main__.data_class'>\n",
            "here11111 torch.Size([32, 1, 28, 28])\n",
            "here <class '__main__.data_class'>\n",
            "here <bound method data_class.shape of <__main__.data_class object at 0x0000024950F3DA00>>\n",
            "here1 <class 'torch.utils.data.dataloader.DataLoader'>\n",
            "here1 <class 'torch.utils.data.dataloader.DataLoader'>\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\laven\\AppData\\Local\\Temp\\ipykernel_3028\\758799266.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.data = torch.stack([torch.tensor(d).float() for d in data])\n",
            "C:\\Users\\laven\\AppData\\Local\\Temp\\ipykernel_3028\\758799266.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.label = torch.stack([torch.tensor(l).long() for l in label])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# The most important class, a custom data loader, understand how it is working.\n",
        "\n",
        "class data_class(Dataset):\n",
        "    def __init__(self,data,label):\n",
        "        self.data = torch.stack([torch.tensor(d).float() for d in data])\n",
        "        self.label = torch.stack([torch.tensor(l).long() for l in label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def  shape(self):\n",
        "      return self.data.shape\n",
        "\n",
        "    def __getitem__(self,id):\n",
        "        data_set=self.data[id]\n",
        "        labels=self.label[id]\n",
        "        return data_set,labels\n",
        "\n",
        "# calling the data_class for the raw random data\n",
        "train_data=data_class(tensor_train_data,label_train)\n",
        "test_data=data_class(tensor_test_data,label_test)\n",
        "print('here',type(train_data))\n",
        "print('here11111',train_data.shape())\n",
        "\n",
        "print('here',type(test_data))\n",
        "print('here',test_data.shape)\n",
        "\n",
        "# Creating the data loader which is going to load the data to the AI model\n",
        "train_dataloader=DataLoader(train_data,batch_size=2,shuffle=True)\n",
        "test_dataloader=DataLoader(test_data,batch_size=2,shuffle=True)\n",
        "print('here1',type(train_dataloader))\n",
        "print('here1',type(test_dataloader))\n",
        "\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIPujNuDfems",
        "outputId": "95f2dd52-cf1b-42ae-982f-469f86d316bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "here11111 torch.Size([71018, 4, 195])\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        }
      ],
      "source": [
        "processed_data = process_sequences('train.txt')\n",
        "#print(processed_data)\n",
        "d = []\n",
        "l = []\n",
        "for i in processed_data:\n",
        "    d.append(i[1])\n",
        "    l.append(i[0])\n",
        "\n",
        "data1 = data_class(data = d, label = l)\n",
        "print('here11111',data1.shape())\n",
        "me=DataLoader(data1,batch_size=512,shuffle=True)\n",
        "#list(train_dataloader)[0:5]\n",
        "# for data,label in train_dataloader:\n",
        "#     print(data, label)\n",
        "#     break\n",
        "\n",
        "print(type(me))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kEtP-qbnRF6"
      },
      "outputs": [],
      "source": [
        "# ########################################CUSTOM DATASET END ################################################################################\n",
        "# ########################################CUSTOM DATASET END ################################################################################\n",
        "# ########################################CUSTOM DATASET END ################################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXmHskFffemp",
        "outputId": "244f61d4-bfe0-4895-bf4f-b8bf63d603a8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ihr_fusLfemq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92Y-xQDSuN6k"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class ProteinCNN(nn.Module):\n",
        "#     def __init__(self, input_dim, output_dim, kernel_size=3, stride=1):\n",
        "#         super(ProteinCNN, self).__init__()\n",
        "#         self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=kernel_size, stride=stride)\n",
        "#         self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=stride)\n",
        "#         self.pool = nn.MaxPool1d(kernel_size=2)  # Max pooling layer with kernel size 2\n",
        "\n",
        "#         # Calculate the input size for the linear layer dynamically\n",
        "#         # by passing a dummy input through the convolutional layers and pooling\n",
        "#         with torch.no_grad():\n",
        "#             dummy_input = torch.randn(1, input_dim, 195)  # Assuming input sequence length is 195\n",
        "#             conv_output = self._forward_conv(dummy_input)\n",
        "#             fc_input_size = conv_output.view(1, -1).size(1)\n",
        "\n",
        "#         self.fc1 = nn.Linear(fc_input_size, 64)\n",
        "#         self.fc2 = nn.Linear(64, output_dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self._forward_conv(x)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return x\n",
        "\n",
        "#     def _forward_conv(self, x):\n",
        "#         x = F.relu(self.conv1(x))\n",
        "#         x = self.pool(x)  # Apply max pooling\n",
        "#         x = F.relu(self.conv2(x))\n",
        "#         x = self.pool(x)  # Apply max pooling\n",
        "#         x = torch.flatten(x, 1)\n",
        "#         return x\n",
        "\n",
        "# # Example usage:\n",
        "# input_dim = 4  # Input dimension\n",
        "# output_dim = 2  # Output dimension\n",
        "# model = ProteinCNN(input_dim, output_dim)\n",
        "\n",
        "\n",
        "\n",
        "# optimizer=optim.Adam(params=model.parameters(),lr=0.0001)\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# # # Generate random protein sequence data\n",
        "# # batch_size = 32\n",
        "# # random_data = torch.randn(batch_size, input_dim, 195)  # Assuming input sequence length is 195\n",
        "# # print('here',random_data.shape)\n",
        "# # print('here',type(random_data))\n",
        "# # # print('here',type(me[]))\n",
        "# # # Forward pass\n",
        "# # output = model(random_data)\n",
        "# # print(output.shape)  # Check output shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctoauKwC4Tuy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ProteinCNN(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, kernel_size=3, stride=1):\n",
        "        super(ProteinCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=64, kernel_size=kernel_size, stride=stride)\n",
        "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=stride)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "\n",
        "        conv_output_size = self._calculate_conv_output_size(input_dim)\n",
        "        self.fc1 = nn.Linear(conv_output_size, 64)\n",
        "        self.fc2 = nn.Linear(64, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self._forward_conv(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "    def _forward_conv(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)  # 应用最大池化\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # 应用最大池化\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "    def _calculate_conv_output_size(self, input_dim):\n",
        "        # 根据输入维度和卷积层参数，手动计算卷积和池化后的输出大小\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.randn(1, input_dim, 195)  # 假设输入序列长度为195\n",
        "            conv_output = self._forward_conv(dummy_input)\n",
        "            return conv_output.size(1)\n",
        "\n",
        "# 示例用法:\n",
        "input_dim = 4  # 输入维度\n",
        "output_dim = 2  # 输出维度\n",
        "model = ProteinCNN(input_dim, output_dim)\n",
        "\n",
        "optimizer=optim.Adam(params=model.parameters(),lr=0.0001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzWY4drmfemp"
      },
      "outputs": [],
      "source": [
        "# class MiniCNN(torch.nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(MiniCNN, self).__init__()\n",
        "#     self.conv1 = torch.nn.Conv2d(1, 2, kernel_size=5)\n",
        "#     self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#     self.conv2 = torch.nn.Conv2d(6, 16, kernel_size=3)\n",
        "#     self.fc1 = torch.nn.Linear(16 * 5 * 5, 120)\n",
        "#     self.fc2 = torch.nn.Linear(120, 2)\n",
        "\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
        "#     x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n",
        "#     x = x.view(-1,16 * 5 * 5)\n",
        "#     x = torch.nn.functional.relu(self.fc1(x))\n",
        "#     x = self.fc2(x)\n",
        "#     return x\n",
        "\n",
        "# # Instantiating the model and assigning an optimizer to the model and creating a loss function\n",
        "# model=MiniCNN().to(device)\n",
        "# optimizer=optim.Adam(params=model.parameters(),lr=0.01)\n",
        "# loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q21JbA9mAzag",
        "outputId": "8cafa0ea-3df8-4208-dbd2-928b9dae973f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 512\n",
            "tar 512\n",
            "inp 362\n",
            "tar 362\n"
          ]
        }
      ],
      "source": [
        "# train_dataloader[0]\n",
        "for inputs, targets in me:\n",
        "    print('inp',len(inputs))\n",
        "    print('tar',len(targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUlfhqw7AOMY",
        "outputId": "7e8e5da0-42fc-406d-8f25-0a4a2d4b0843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "tensor([[[[0.0137, 0.2064, 0.4486,  ..., 0.2225, 0.6973, 0.2446],\n",
            "          [0.1447, 0.2664, 0.7716,  ..., 0.1073, 0.8519, 0.9997],\n",
            "          [0.8602, 0.3662, 0.1629,  ..., 0.2187, 0.5497, 0.0478],\n",
            "          ...,\n",
            "          [0.6772, 0.7639, 0.2029,  ..., 0.8974, 0.2007, 0.9820],\n",
            "          [0.0293, 0.2588, 0.4604,  ..., 0.0657, 0.1910, 0.4991],\n",
            "          [0.2831, 0.7301, 0.9130,  ..., 0.7302, 0.1375, 0.6223]]],\n",
            "\n",
            "\n",
            "        [[[0.4841, 0.4923, 0.8459,  ..., 0.3469, 0.0568, 0.4097],\n",
            "          [0.6246, 0.0043, 0.0538,  ..., 0.8119, 0.7520, 0.5862],\n",
            "          [0.3620, 0.6916, 0.9552,  ..., 0.3302, 0.1988, 0.0803],\n",
            "          ...,\n",
            "          [0.7805, 0.4941, 0.9388,  ..., 0.2194, 0.2433, 0.5434],\n",
            "          [0.8073, 0.1699, 0.3576,  ..., 0.6347, 0.4458, 0.6849],\n",
            "          [0.8117, 0.1212, 0.1408,  ..., 0.9087, 0.0406, 0.8006]]]])\n",
            "class 2\n",
            "\n",
            "1\n",
            "tensor([[[[0.2484, 0.7729, 0.8939,  ..., 0.1629, 0.4794, 0.4668],\n",
            "          [0.7945, 0.2258, 0.0931,  ..., 0.5510, 0.7559, 0.1503],\n",
            "          [0.0393, 0.3247, 0.2435,  ..., 0.5215, 0.3019, 0.9770],\n",
            "          ...,\n",
            "          [0.6320, 0.4828, 0.9108,  ..., 0.9528, 0.1851, 0.6432],\n",
            "          [0.1804, 0.8119, 0.7525,  ..., 0.8384, 0.4803, 0.5912],\n",
            "          [0.4751, 0.4607, 0.6485,  ..., 0.3140, 0.2815, 0.7177]]],\n",
            "\n",
            "\n",
            "        [[[0.5226, 0.1245, 0.2688,  ..., 0.1454, 0.6435, 0.9384],\n",
            "          [0.8276, 0.0854, 0.4225,  ..., 0.9178, 0.6451, 0.3739],\n",
            "          [0.8600, 0.0591, 0.4421,  ..., 0.1049, 0.4834, 0.9788],\n",
            "          ...,\n",
            "          [0.8689, 0.0272, 0.9635,  ..., 0.0468, 0.4114, 0.0812],\n",
            "          [0.6201, 0.6788, 0.6510,  ..., 0.0351, 0.8431, 0.0695],\n",
            "          [0.1992, 0.6885, 0.0360,  ..., 0.4942, 0.9193, 0.5614]]]])\n",
            "class 2\n",
            "\n",
            "2\n",
            "tensor([[[[0.4216, 0.7996, 0.9648,  ..., 0.6801, 0.3645, 0.4335],\n",
            "          [0.5077, 0.0026, 0.8105,  ..., 0.4275, 0.7074, 0.0467],\n",
            "          [0.7690, 0.8900, 0.9810,  ..., 0.6341, 0.1384, 0.7854],\n",
            "          ...,\n",
            "          [0.9659, 0.8025, 0.9719,  ..., 0.1093, 0.7035, 0.9544],\n",
            "          [0.9272, 0.3053, 0.8638,  ..., 0.1879, 0.2985, 0.4674],\n",
            "          [0.3408, 0.6910, 0.7223,  ..., 0.4193, 0.1922, 0.6772]]],\n",
            "\n",
            "\n",
            "        [[[0.7128, 0.1212, 0.4913,  ..., 0.8809, 0.9154, 0.9405],\n",
            "          [0.2613, 0.9124, 0.8397,  ..., 0.5407, 0.6700, 0.0173],\n",
            "          [0.8035, 0.0509, 0.3892,  ..., 0.6547, 0.3918, 0.8296],\n",
            "          ...,\n",
            "          [0.6364, 0.2476, 0.1216,  ..., 0.3075, 0.2326, 0.4625],\n",
            "          [0.1533, 0.7902, 0.6392,  ..., 0.1916, 0.9790, 0.3223],\n",
            "          [0.4974, 0.8489, 0.7940,  ..., 0.5430, 0.9529, 0.7068]]]])\n",
            "class 2\n",
            "\n",
            "3\n",
            "tensor([[[[0.0724, 0.1251, 0.2363,  ..., 0.2557, 0.6973, 0.7524],\n",
            "          [0.6851, 0.8921, 0.0292,  ..., 0.7295, 0.0606, 0.2284],\n",
            "          [0.6581, 0.6776, 0.6405,  ..., 0.5637, 0.4389, 0.9584],\n",
            "          ...,\n",
            "          [0.1284, 0.0846, 0.3384,  ..., 0.8467, 0.4293, 0.6014],\n",
            "          [0.1346, 0.4513, 0.6152,  ..., 0.8473, 0.4564, 0.2695],\n",
            "          [0.1913, 0.1119, 0.8646,  ..., 0.3195, 0.1539, 0.6150]]],\n",
            "\n",
            "\n",
            "        [[[0.5480, 0.1887, 0.4476,  ..., 0.5259, 0.8355, 0.6338],\n",
            "          [0.2939, 0.8747, 0.9427,  ..., 0.5933, 0.5600, 0.5345],\n",
            "          [0.9553, 0.8940, 0.4590,  ..., 0.4566, 0.4108, 0.0054],\n",
            "          ...,\n",
            "          [0.0935, 0.6297, 0.5531,  ..., 0.1629, 0.6273, 0.1225],\n",
            "          [0.4238, 0.7402, 0.6487,  ..., 0.9653, 0.3879, 0.7007],\n",
            "          [0.5110, 0.7395, 0.4413,  ..., 0.8192, 0.0289, 0.4143]]]])\n",
            "class 2\n",
            "\n",
            "4\n",
            "tensor([[[[0.5241, 0.0237, 0.0283,  ..., 0.6130, 0.0909, 0.8739],\n",
            "          [0.7245, 0.9999, 0.4924,  ..., 0.1901, 0.0467, 0.5630],\n",
            "          [0.8987, 0.1112, 0.2980,  ..., 0.3607, 0.0347, 0.5457],\n",
            "          ...,\n",
            "          [0.7043, 0.5222, 0.8435,  ..., 0.7290, 0.2315, 0.0776],\n",
            "          [0.6111, 0.8283, 0.3480,  ..., 0.6434, 0.5293, 0.1567],\n",
            "          [0.7277, 0.4522, 0.1819,  ..., 0.4392, 0.4945, 0.4847]]],\n",
            "\n",
            "\n",
            "        [[[0.3339, 0.7109, 0.8615,  ..., 0.7052, 0.9815, 0.7992],\n",
            "          [0.9496, 0.2429, 0.5154,  ..., 0.5468, 0.8028, 0.8605],\n",
            "          [0.0266, 0.1454, 0.4815,  ..., 0.1668, 0.6915, 0.7591],\n",
            "          ...,\n",
            "          [0.1832, 0.6990, 0.4692,  ..., 0.4243, 0.4782, 0.7406],\n",
            "          [0.4165, 0.8783, 0.2392,  ..., 0.4263, 0.1338, 0.0835],\n",
            "          [0.9291, 0.5337, 0.0508,  ..., 0.9875, 0.8348, 0.9320]]]])\n",
            "class 2\n",
            "\n",
            "5\n",
            "tensor([[[[8.0961e-01, 8.9426e-02, 6.8676e-01,  ..., 9.6461e-01,\n",
            "           5.5468e-01, 6.8445e-01],\n",
            "          [8.8698e-01, 4.1041e-02, 3.2633e-01,  ..., 6.8075e-01,\n",
            "           3.2767e-01, 2.8508e-01],\n",
            "          [9.7050e-01, 1.2025e-01, 9.5525e-01,  ..., 3.4229e-01,\n",
            "           4.8946e-01, 2.1004e-01],\n",
            "          ...,\n",
            "          [2.9440e-01, 9.3938e-01, 2.0307e-01,  ..., 5.9763e-01,\n",
            "           1.9207e-01, 3.9370e-02],\n",
            "          [4.5917e-01, 4.8819e-02, 9.3139e-01,  ..., 5.2085e-01,\n",
            "           7.5288e-01, 2.1653e-01],\n",
            "          [2.0727e-02, 4.6241e-01, 2.4961e-01,  ..., 9.4058e-01,\n",
            "           2.7712e-04, 2.0512e-01]]],\n",
            "\n",
            "\n",
            "        [[[3.7681e-01, 4.0437e-01, 3.7279e-01,  ..., 2.1121e-01,\n",
            "           8.4546e-01, 5.7584e-02],\n",
            "          [8.8647e-01, 2.6794e-01, 1.0785e-01,  ..., 8.6208e-01,\n",
            "           6.8164e-01, 9.7552e-01],\n",
            "          [9.9574e-02, 2.3316e-02, 6.5600e-02,  ..., 7.4175e-01,\n",
            "           7.1417e-02, 7.5303e-01],\n",
            "          ...,\n",
            "          [4.9936e-01, 2.5633e-01, 8.0683e-01,  ..., 9.0344e-03,\n",
            "           3.9040e-01, 3.5101e-01],\n",
            "          [9.1378e-02, 5.8664e-01, 3.8770e-01,  ..., 3.2907e-01,\n",
            "           5.9564e-01, 4.9311e-01],\n",
            "          [3.9734e-01, 8.6072e-01, 4.7834e-01,  ..., 7.4760e-01,\n",
            "           3.7546e-01, 8.1536e-01]]]])\n",
            "class 2\n",
            "\n",
            "6\n",
            "tensor([[[[0.7284, 0.9508, 0.5505,  ..., 0.2304, 0.8333, 0.2694],\n",
            "          [0.0870, 0.0064, 0.7584,  ..., 0.0867, 0.1016, 0.0280],\n",
            "          [0.8949, 0.8472, 0.5772,  ..., 0.3449, 0.6057, 0.8452],\n",
            "          ...,\n",
            "          [0.9207, 0.7686, 0.4509,  ..., 0.9773, 0.0266, 0.7554],\n",
            "          [0.6042, 0.4037, 0.7455,  ..., 0.5520, 0.5942, 0.6230],\n",
            "          [0.7523, 0.7423, 0.0143,  ..., 0.8534, 0.3695, 0.8796]]],\n",
            "\n",
            "\n",
            "        [[[0.7341, 0.6642, 0.0399,  ..., 0.7326, 0.0481, 0.3051],\n",
            "          [0.6093, 0.9323, 0.0316,  ..., 0.2581, 0.7586, 0.4980],\n",
            "          [0.8421, 0.8842, 0.1760,  ..., 0.4003, 0.4136, 0.0876],\n",
            "          ...,\n",
            "          [0.7998, 0.9406, 0.8358,  ..., 0.7567, 0.7867, 0.0439],\n",
            "          [0.6776, 0.9254, 0.4816,  ..., 0.1816, 0.0516, 0.5480],\n",
            "          [0.2230, 0.3879, 0.7338,  ..., 0.1383, 0.1343, 0.4796]]]])\n",
            "class 2\n",
            "\n",
            "7\n",
            "tensor([[[[1.5325e-01, 3.3041e-01, 5.4790e-01,  ..., 6.4781e-01,\n",
            "           8.7804e-01, 4.0739e-01],\n",
            "          [4.1526e-01, 4.2229e-01, 1.9664e-01,  ..., 5.2135e-02,\n",
            "           6.4244e-01, 9.9619e-01],\n",
            "          [2.4492e-01, 6.0428e-01, 4.2986e-01,  ..., 8.4849e-01,\n",
            "           6.6283e-01, 3.9090e-01],\n",
            "          ...,\n",
            "          [2.2384e-01, 3.0726e-01, 7.1079e-01,  ..., 8.4494e-01,\n",
            "           6.3716e-01, 6.8822e-01],\n",
            "          [7.0618e-04, 7.6574e-02, 9.2153e-01,  ..., 6.7864e-01,\n",
            "           4.3791e-01, 4.2353e-01],\n",
            "          [3.2091e-01, 7.7585e-01, 9.2401e-03,  ..., 1.4173e-02,\n",
            "           4.2734e-01, 4.7245e-01]]],\n",
            "\n",
            "\n",
            "        [[[6.5192e-01, 9.1735e-01, 4.2023e-01,  ..., 1.0736e-01,\n",
            "           1.7847e-01, 4.8543e-01],\n",
            "          [5.1260e-01, 7.1153e-01, 7.8173e-03,  ..., 4.1610e-01,\n",
            "           6.3924e-01, 3.1485e-02],\n",
            "          [6.1334e-01, 1.5496e-01, 8.8767e-01,  ..., 7.1261e-02,\n",
            "           5.2777e-01, 3.5669e-01],\n",
            "          ...,\n",
            "          [8.2936e-01, 1.1384e-01, 1.6923e-01,  ..., 3.6070e-01,\n",
            "           3.9386e-01, 5.0932e-01],\n",
            "          [4.1229e-01, 1.7522e-02, 6.0255e-02,  ..., 6.2150e-01,\n",
            "           8.9905e-01, 6.8083e-01],\n",
            "          [3.0654e-01, 9.2775e-01, 9.9491e-01,  ..., 9.4304e-01,\n",
            "           9.6384e-01, 1.0671e-01]]]])\n",
            "class 2\n",
            "\n",
            "8\n",
            "tensor([[[[0.2421, 0.6209, 0.5911,  ..., 0.3645, 0.0196, 0.5979],\n",
            "          [0.6132, 0.1502, 0.4371,  ..., 0.6893, 0.2286, 0.9708],\n",
            "          [0.4117, 0.8627, 0.2678,  ..., 0.3002, 0.4748, 0.3614],\n",
            "          ...,\n",
            "          [0.9864, 0.9713, 0.1217,  ..., 0.8894, 0.7914, 0.8506],\n",
            "          [0.9474, 0.5380, 0.6157,  ..., 0.2600, 0.7061, 0.7611],\n",
            "          [0.7250, 0.8702, 0.9685,  ..., 0.6917, 0.3251, 0.5122]]],\n",
            "\n",
            "\n",
            "        [[[0.8014, 0.2387, 0.4850,  ..., 0.4491, 0.3747, 0.0816],\n",
            "          [0.1394, 0.2641, 0.1282,  ..., 0.3904, 0.0468, 0.0173],\n",
            "          [0.6842, 0.7425, 0.3897,  ..., 0.7668, 0.1141, 0.3866],\n",
            "          ...,\n",
            "          [0.6715, 0.7333, 0.9968,  ..., 0.7889, 0.8855, 0.1371],\n",
            "          [0.6601, 0.6665, 0.9162,  ..., 0.5807, 0.5891, 0.2499],\n",
            "          [0.1676, 0.7004, 0.2054,  ..., 0.9822, 0.1568, 0.3913]]]])\n",
            "class 2\n",
            "\n",
            "9\n",
            "tensor([[[[0.2811, 0.4844, 0.7237,  ..., 0.2440, 0.6904, 0.5782],\n",
            "          [0.5897, 0.1125, 0.0862,  ..., 0.3103, 0.1078, 0.1418],\n",
            "          [0.2758, 0.7653, 0.1943,  ..., 0.5707, 0.7186, 0.5485],\n",
            "          ...,\n",
            "          [0.0076, 0.1564, 0.5308,  ..., 0.6599, 0.7196, 0.9262],\n",
            "          [0.1048, 0.4634, 0.1530,  ..., 0.1570, 0.4676, 0.6889],\n",
            "          [0.5021, 0.0536, 0.2297,  ..., 0.3070, 0.3171, 0.1134]]],\n",
            "\n",
            "\n",
            "        [[[0.9779, 0.3732, 0.3021,  ..., 0.3100, 0.6203, 0.7937],\n",
            "          [0.3707, 0.9345, 0.8588,  ..., 0.3882, 0.1860, 0.9157],\n",
            "          [0.9632, 0.4438, 0.3276,  ..., 0.1365, 0.9351, 0.4949],\n",
            "          ...,\n",
            "          [0.3371, 0.6096, 0.4151,  ..., 0.9478, 0.5431, 0.3587],\n",
            "          [0.7294, 0.5243, 0.9316,  ..., 0.6001, 0.4182, 0.7595],\n",
            "          [0.2750, 0.4550, 0.4789,  ..., 0.0671, 0.9912, 0.7745]]]])\n",
            "class 2\n",
            "\n",
            "10\n",
            "tensor([[[[0.7085, 0.1171, 0.6082,  ..., 0.6002, 0.8184, 0.5737],\n",
            "          [0.8568, 0.3625, 0.5370,  ..., 0.7437, 0.8017, 0.7082],\n",
            "          [0.5029, 0.6371, 0.3480,  ..., 0.3132, 0.6718, 0.3930],\n",
            "          ...,\n",
            "          [0.0943, 0.7830, 0.2777,  ..., 0.7011, 0.7021, 0.1912],\n",
            "          [0.3346, 0.9627, 0.6355,  ..., 0.1764, 0.9165, 0.8909],\n",
            "          [0.7516, 0.6840, 0.3759,  ..., 0.5361, 0.7482, 0.3208]]],\n",
            "\n",
            "\n",
            "        [[[0.7620, 0.6542, 0.3092,  ..., 0.3554, 0.3530, 0.0117],\n",
            "          [0.8665, 0.5013, 0.1267,  ..., 0.9640, 0.5488, 0.3372],\n",
            "          [0.3463, 0.0567, 0.7734,  ..., 0.9477, 0.6490, 0.5371],\n",
            "          ...,\n",
            "          [0.1093, 0.5438, 0.7722,  ..., 0.1797, 0.8837, 0.4174],\n",
            "          [0.0608, 0.8917, 0.6365,  ..., 0.3997, 0.7270, 0.9543],\n",
            "          [0.2955, 0.4095, 0.2179,  ..., 0.9028, 0.2089, 0.3420]]]])\n",
            "class 2\n",
            "\n",
            "11\n",
            "tensor([[[[0.5800, 0.1369, 0.5406,  ..., 0.2407, 0.2013, 0.9219],\n",
            "          [0.0848, 0.5804, 0.1554,  ..., 0.9392, 0.4009, 0.4084],\n",
            "          [0.5169, 0.0267, 0.0731,  ..., 0.2500, 0.1493, 0.3830],\n",
            "          ...,\n",
            "          [0.7613, 0.0019, 0.5852,  ..., 0.3319, 0.8756, 0.9535],\n",
            "          [0.3784, 0.5664, 0.3223,  ..., 0.1186, 0.3331, 0.2033],\n",
            "          [0.6527, 0.3376, 0.7372,  ..., 0.4061, 0.8688, 0.7039]]],\n",
            "\n",
            "\n",
            "        [[[0.8993, 0.1852, 0.6392,  ..., 0.8381, 0.7424, 0.3267],\n",
            "          [0.4811, 0.8799, 0.3613,  ..., 0.4024, 0.4687, 0.0900],\n",
            "          [0.0663, 0.4928, 0.4219,  ..., 0.3010, 0.7456, 0.3112],\n",
            "          ...,\n",
            "          [0.6908, 0.6983, 0.5011,  ..., 0.1221, 0.7931, 0.2118],\n",
            "          [0.0910, 0.8543, 0.2688,  ..., 0.6863, 0.1644, 0.7745],\n",
            "          [0.1753, 0.1537, 0.1431,  ..., 0.4746, 0.8991, 0.5741]]]])\n",
            "class 2\n",
            "\n",
            "12\n",
            "tensor([[[[0.0291, 0.9579, 0.6330,  ..., 0.9227, 0.4288, 0.1048],\n",
            "          [0.0648, 0.0253, 0.5300,  ..., 0.2300, 0.7721, 0.0472],\n",
            "          [0.4042, 0.9187, 0.9599,  ..., 0.4389, 0.3900, 0.3978],\n",
            "          ...,\n",
            "          [0.5173, 0.6152, 0.9842,  ..., 0.7798, 0.3400, 0.7056],\n",
            "          [0.9030, 0.2934, 0.8227,  ..., 0.5541, 0.8475, 0.0143],\n",
            "          [0.4345, 0.6979, 0.5752,  ..., 0.8767, 0.6412, 0.7122]]],\n",
            "\n",
            "\n",
            "        [[[0.5860, 0.1413, 0.8505,  ..., 0.0803, 0.1486, 0.4792],\n",
            "          [0.3829, 0.6227, 0.1812,  ..., 0.6951, 0.1021, 0.6284],\n",
            "          [0.7782, 0.7563, 0.5039,  ..., 0.1591, 0.4229, 0.6401],\n",
            "          ...,\n",
            "          [0.0499, 0.7031, 0.7676,  ..., 0.1897, 0.3818, 0.8775],\n",
            "          [0.7528, 0.6071, 0.9160,  ..., 0.2564, 0.9602, 0.4860],\n",
            "          [0.3189, 0.2989, 0.8596,  ..., 0.3687, 0.0926, 0.8814]]]])\n",
            "class 2\n",
            "\n",
            "13\n",
            "tensor([[[[0.4504, 0.7943, 0.8853,  ..., 0.4124, 0.0713, 0.3991],\n",
            "          [0.8923, 0.2645, 0.0121,  ..., 0.6314, 0.9915, 0.8187],\n",
            "          [0.3877, 0.3822, 0.4570,  ..., 0.0720, 0.2291, 0.0723],\n",
            "          ...,\n",
            "          [0.9668, 0.5328, 0.4827,  ..., 0.7771, 0.8319, 0.5874],\n",
            "          [0.5168, 0.1533, 0.6549,  ..., 0.7441, 0.4395, 0.7495],\n",
            "          [0.2992, 0.7635, 0.6022,  ..., 0.4957, 0.7521, 0.9835]]],\n",
            "\n",
            "\n",
            "        [[[0.0586, 0.2483, 0.8203,  ..., 0.0849, 0.8161, 0.7223],\n",
            "          [0.1955, 0.2268, 0.5588,  ..., 0.9127, 0.8061, 0.7484],\n",
            "          [0.1703, 0.9824, 0.2468,  ..., 0.2773, 0.9650, 0.5129],\n",
            "          ...,\n",
            "          [0.2899, 0.8066, 0.5243,  ..., 0.7043, 0.3363, 0.1099],\n",
            "          [0.6721, 0.5282, 0.4380,  ..., 0.7910, 0.4742, 0.2848],\n",
            "          [0.4633, 0.1397, 0.4808,  ..., 0.4787, 0.2357, 0.4197]]]])\n",
            "class 2\n",
            "\n",
            "14\n",
            "tensor([[[[0.0657, 0.8614, 0.4072,  ..., 0.0281, 0.9766, 0.9234],\n",
            "          [0.8451, 0.8418, 0.6905,  ..., 0.0364, 0.0057, 0.7941],\n",
            "          [0.0223, 0.5537, 0.5636,  ..., 0.9110, 0.7564, 0.7294],\n",
            "          ...,\n",
            "          [0.1717, 0.3014, 0.0529,  ..., 0.4112, 0.5095, 0.5668],\n",
            "          [0.4520, 0.7672, 0.9702,  ..., 0.1937, 0.9543, 0.3172],\n",
            "          [0.9147, 0.5767, 0.9468,  ..., 0.4150, 0.6332, 0.2684]]],\n",
            "\n",
            "\n",
            "        [[[0.3376, 0.6682, 0.9558,  ..., 0.9138, 0.1020, 0.1983],\n",
            "          [0.8450, 0.3023, 0.9755,  ..., 0.5040, 0.2399, 0.4548],\n",
            "          [0.0393, 0.3183, 0.5454,  ..., 0.3229, 0.4964, 0.5548],\n",
            "          ...,\n",
            "          [0.5742, 0.5021, 0.5949,  ..., 0.5912, 0.3692, 0.0861],\n",
            "          [0.8255, 0.8731, 0.7884,  ..., 0.8311, 0.4580, 0.9192],\n",
            "          [0.1793, 0.0070, 0.2769,  ..., 0.6099, 0.9371, 0.1334]]]])\n",
            "class 2\n",
            "\n",
            "15\n",
            "tensor([[[[0.5466, 0.5694, 0.2084,  ..., 0.2918, 0.7602, 0.6757],\n",
            "          [0.8951, 0.6916, 0.0671,  ..., 0.4896, 0.9957, 0.7026],\n",
            "          [0.0866, 0.6152, 0.6140,  ..., 0.0656, 0.7479, 0.4910],\n",
            "          ...,\n",
            "          [0.9033, 0.5792, 0.8217,  ..., 0.1524, 0.4045, 0.5310],\n",
            "          [0.4486, 0.7623, 0.5921,  ..., 0.4459, 0.0689, 0.0598],\n",
            "          [0.5114, 0.2111, 0.9822,  ..., 0.9323, 0.5720, 0.2690]]],\n",
            "\n",
            "\n",
            "        [[[0.0285, 0.0096, 0.1188,  ..., 0.6528, 0.1363, 0.8366],\n",
            "          [0.5737, 0.9006, 0.1573,  ..., 0.1468, 0.7358, 0.2264],\n",
            "          [0.5796, 0.3078, 0.5575,  ..., 0.1440, 0.8093, 0.2516],\n",
            "          ...,\n",
            "          [0.5641, 0.6828, 0.2892,  ..., 0.2983, 0.7336, 0.7065],\n",
            "          [0.1369, 0.0479, 0.5923,  ..., 0.0761, 0.5476, 0.1506],\n",
            "          [0.4278, 0.9805, 0.1990,  ..., 0.5620, 0.8918, 0.3321]]]])\n",
            "class 2\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for batch_ids, (img, classes) in enumerate(train_dataloader):\n",
        "    print(batch_ids)\n",
        "    print(img)\n",
        "    print('class',len(classes))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jimXzVRpfemr"
      },
      "outputs": [],
      "source": [
        "def train(model,device,train_dataloader,optimizer,epochs):\n",
        "    print(\"inside train\")\n",
        "    model.train()\n",
        "    for batch_ids, (img, classes) in enumerate(train_dataloader):\n",
        "        # print(batch_ids)\n",
        "        # print(img)\n",
        "        # print('class',len(classes))\n",
        "        # print()\n",
        "        # classes=classes.type(torch.LongTensor)\n",
        "        img,classes=img.to(device),classes.to(device)\n",
        "        torch.autograd.set_detect_anomaly(True)\n",
        "        optimizer.zero_grad()\n",
        "        output=model(img)\n",
        "        loss = loss_fn(output,classes)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if(batch_ids) % 2 == 0:\n",
        "        print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "            epochs, batch_ids* len(img), len(train_dataloader.dataset),\n",
        "            100.*batch_ids / len(train_dataloader),loss.item()))\n",
        "\n",
        "def test(model, device, test_dataloader):\n",
        "    model.eval()\n",
        "    test_loss=0\n",
        "    correct=0\n",
        "    with torch.no_grad():\n",
        "        for img,classes in test_dataloader:\n",
        "            img,classes=img.to(device), classes.to(device)\n",
        "            y_hat=model(img)\n",
        "            test_loss+=F.nll_loss(y_hat,classes,reduction='sum').item()\n",
        "            _,y_pred=torch.max(y_hat,1)\n",
        "            correct+=(y_pred==classes).sum().item()\n",
        "        test_loss/=len(test_dataloader)\n",
        "        print(\"\\n Test set: Avarage loss: {:.0f},Accuracy:{}/{} ({:.0f}%)\\n\".format(\n",
        "            test_loss,correct,len(test_dataloader),100.*correct/len(test_dataloader)))\n",
        "        print('='*30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs9HhK3Qfems",
        "outputId": "481d5c91-f72d-4e47-f949-05f03b5621b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inside train\n",
            "Train Epoch: 1 [49956/71018 (99%)]\tLoss: 0.623651\n",
            "inside train\n",
            "Train Epoch: 2 [49956/71018 (99%)]\tLoss: 0.609510\n"
          ]
        }
      ],
      "source": [
        "# WE ARE USING RANDOM DATA SO THE TRAINING AND TESTING DOES NOT MATTER, THE AIM IS TO SHOWCASE THE USE OF A CUSTOM DATASET\n",
        "# SINCE IN PRACTICAL SENSE YOU HAVE TO CLEAN THE DATA AND LOAD THE DATA INTO THE MODEL.\n",
        "if __name__=='__main__':\n",
        "    seed=42\n",
        "    EPOCHS=10\n",
        "    model = model.cuda()\n",
        "    for epoch in range(1,EPOCHS+1):\n",
        "        train(model,device,me,optimizer,epoch)\n",
        "        # test(model,device,test_dataloader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
